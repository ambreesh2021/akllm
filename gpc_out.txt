(base) jupyter@pytorch-1-13-20230422-104511:~$ python3.7 gpt_dev_scaled.py 
step 0: loss = 4.4753, val_loss = 4.4709                                                                                                                                     
step 10: loss = 2.9867, val_loss = 3.0120                                                                                                                                    
step 20: loss = 2.7395, val_loss = 2.7668                                                                                                                                    
step 30: loss = 2.6369, val_loss = 2.6575                                                                                                                                    
step 40: loss = 2.5805, val_loss = 2.6032ion(4, n_embd//4) # i.e. 4 heads of 8 dimeansional self attention                                                                   
step 50: loss = 2.5470, val_loss = 2.5742                                                                                                                                    
step 60: loss = 2.5258, val_loss = 2.5510vocab_size)
step 70: loss = 2.5108, val_loss = 2.5374
step 80: loss = 2.5006, val_loss = 2.5180
step 90: loss = 2.4922, val_loss = 2.5146
step 100: loss = 2.4856, val_loss = 2.5047eight, mean=0.0, std=0.02)
step 110: loss = 2.4797, val_loss = 2.4985
step 120: loss = 2.4734, val_loss = 2.4974e.bias)
step 130: loss = 2.4694, val_loss = 2.4944ng):
step 140: loss = 2.4662, val_loss = 2.4869eight, mean=0.0, std=0.02)
step 150: loss = 2.4615, val_loss = 2.4845
step 160: loss = 2.4588, val_loss = 2.4800
step 170: loss = 2.4515, val_loss = 2.4738
step 180: loss = 2.4482, val_loss = 2.4743ensors of integers                                                                                                                 
step 190: loss = 2.4463, val_loss = 2.4661le(idx) # (B,T,C)                                                                                                                  
step 200: loss = 2.4413, val_loss = 2.4665table(torch.arange(T, device=device)) # (T,C)                                                                                      
step 210: loss = 2.4373, val_loss = 2.4625                                                                                                                                   
step 220: loss = 2.4320, val_loss = 2.4563                                                                                                                                   
step 230: loss = 2.4285, val_loss = 2.4572 head of self-attention. (B,T,C)                                                                                                   
step 240: loss = 2.4226, val_loss = 2.4551                                                                                                                                   
step 250: loss = 2.4203, val_loss = 2.4441cab_size)                                                                                                                          
step 260: loss = 2.4190, val_loss = 2.4439
step 270: loss = 2.4175, val_loss = 2.4394
step 280: loss = 2.4080, val_loss = 2.43233
step 290: loss = 2.3987, val_loss = 2.4277
step 300: loss = 2.3930, val_loss = 2.4257
step 310: loss = 2.3869, val_loss = 2.4159
step 320: loss = 2.3787, val_loss = 2.4095
step 330: loss = 2.3731, val_loss = 2.4094
step 340: loss = 2.3630, val_loss = 2.3991
step 350: loss = 2.3530, val_loss = 2.3857
step 360: loss = 2.3405, val_loss = 2.3742
step 370: loss = 2.3273, val_loss = 2.3651
step 380: loss = 2.3075, val_loss = 2.3450
step 390: loss = 2.2956, val_loss = 2.3298
step 400: loss = 2.2759, val_loss = 2.3096
step 410: loss = 2.2579, val_loss = 2.3013
step 420: loss = 2.2380, val_loss = 2.2773
step 430: loss = 2.2112, val_loss = 2.2607
step 440: loss = 2.1968, val_loss = 2.2439
step 450: loss = 2.1737, val_loss = 2.2239
step 460: loss = 2.1548, val_loss = 2.2060
step 470: loss = 2.1365, val_loss = 2.1889
step 480: loss = 2.1165, val_loss = 2.1721
step 490: loss = 2.1035, val_loss = 2.1619
step 500: loss = 2.0815, val_loss = 2.1479
step 510: loss = 2.0711, val_loss = 2.1362
step 520: loss = 2.0595, val_loss = 2.1319
step 530: loss = 2.0460, val_loss = 2.1198
step 540: loss = 2.0319, val_loss = 2.1078
step 550: loss = 2.0159, val_loss = 2.1010
step 560: loss = 2.0064, val_loss = 2.0871
step 570: loss = 1.9928, val_loss = 2.0763
step 580: loss = 1.9846, val_loss = 2.0775
step 590: loss = 1.9717, val_loss = 2.0670
step 600: loss = 1.9672, val_loss = 2.0616
step 610: loss = 1.9523, val_loss = 2.0501
step 620: loss = 1.9366, val_loss = 2.0362
step 630: loss = 1.9265, val_loss = 2.0293
step 640: loss = 1.9197, val_loss = 2.0175
step 650: loss = 1.9102, val_loss = 2.0099
step 660: loss = 1.9031, val_loss = 2.0103
step 670: loss = 1.8959, val_loss = 2.0073
step 680: loss = 1.8796, val_loss = 1.9934
step 690: loss = 1.8781, val_loss = 1.9957
step 700: loss = 1.8603, val_loss = 1.9763
step 710: loss = 1.8503, val_loss = 1.9748
step 720: loss = 1.8461, val_loss = 1.9609
step 730: loss = 1.8336, val_loss = 1.9536
step 740: loss = 1.8280, val_loss = 1.9580
step 750: loss = 1.8207, val_loss = 1.9403
step 760: loss = 1.8149, val_loss = 1.9416
step 770: loss = 1.8058, val_loss = 1.9313
step 780: loss = 1.7953, val_loss = 1.9260
step 790: loss = 1.7854, val_loss = 1.9192
step 800: loss = 1.7747, val_loss = 1.9125
step 810: loss = 1.7682, val_loss = 1.9069
step 820: loss = 1.7672, val_loss = 1.9005
step 830: loss = 1.7530, val_loss = 1.8923
step 840: loss = 1.7506, val_loss = 1.8947
step 850: loss = 1.7477, val_loss = 1.8901
step 860: loss = 1.7359, val_loss = 1.8734
step 870: loss = 1.7303, val_loss = 1.8690
step 880: loss = 1.7304, val_loss = 1.8757
step 890: loss = 1.7189, val_loss = 1.8683
step 900: loss = 1.7113, val_loss = 1.8578
^[[>0;276;0c^[]11;rgb:ffff/ffff/ffff^[\^[[>0;276;0c^[]11;rgb:ffff/ffff/ffff^[\^[[>0;276;0c^[]11;rgb:ffff/ffff/ffff^[\step 910: loss = 1.7115, val_loss = 1.8604
step 920: loss = 1.7026, val_loss = 1.8559
step 930: loss = 1.6941, val_loss = 1.8467
step 940: loss = 1.6909, val_loss = 1.8418
step 950: loss = 1.6874, val_loss = 1.8510
step 960: loss = 1.6764, val_loss = 1.8347
step 970: loss = 1.6790, val_loss = 1.8374
step 980: loss = 1.6709, val_loss = 1.8279
step 990: loss = 1.6657, val_loss = 1.8230
step 1000: loss = 1.6555, val_loss = 1.8154
step 1010: loss = 1.6552, val_loss = 1.8127
step 1020: loss = 1.6485, val_loss = 1.8130
step 1030: loss = 1.6418, val_loss = 1.8051
step 1040: loss = 1.6373, val_loss = 1.8043
step 1050: loss = 1.6337, val_loss = 1.8040
step 1060: loss = 1.6308, val_loss = 1.8000
step 1070: loss = 1.6321, val_loss = 1.8015
step 1080: loss = 1.6236, val_loss = 1.7930
step 1090: loss = 1.6158, val_loss = 1.7866
step 1100: loss = 1.6163, val_loss = 1.7814
step 1110: loss = 1.6111, val_loss = 1.7876
step 1120: loss = 1.6038, val_loss = 1.7739
step 1130: loss = 1.6005, val_loss = 1.7649
step 1140: loss = 1.5960, val_loss = 1.7682
step 1150: loss = 1.5945, val_loss = 1.7709
step 1160: loss = 1.5903, val_loss = 1.7620
step 1170: loss = 1.5863, val_loss = 1.7578
step 1180: loss = 1.5799, val_loss = 1.7516
step 1190: loss = 1.5811, val_loss = 1.7526
step 1200: loss = 1.5737, val_loss = 1.7505
step 1210: loss = 1.5716, val_loss = 1.7445
step 1220: loss = 1.5674, val_loss = 1.7495
step 1230: loss = 1.5657, val_loss = 1.7443
step 1240: loss = 1.5589, val_loss = 1.7422
step 1250: loss = 1.5562, val_loss = 1.7398
step 1260: loss = 1.5541, val_loss = 1.7330
step 1270: loss = 1.5468, val_loss = 1.7243
step 1280: loss = 1.5430, val_loss = 1.7266
step 1290: loss = 1.5414, val_loss = 1.7282
step 1300: loss = 1.5402, val_loss = 1.7267
step 1310: loss = 1.5413, val_loss = 1.7269
step 1320: loss = 1.5399, val_loss = 1.7251
step 1330: loss = 1.5313, val_loss = 1.7186
step 1340: loss = 1.5302, val_loss = 1.7230
step 1350: loss = 1.5295, val_loss = 1.7245
step 1360: loss = 1.5207, val_loss = 1.7162
step 1370: loss = 1.5229, val_loss = 1.7104
step 1380: loss = 1.5165, val_loss = 1.7105
step 1390: loss = 1.5137, val_loss = 1.7104
step 1400: loss = 1.5158, val_loss = 1.7090
step 1410: loss = 1.5088, val_loss = 1.7036
step 1420: loss = 1.5119, val_loss = 1.7048
step 1430: loss = 1.5106, val_loss = 1.7024
step 1440: loss = 1.5014, val_loss = 1.6945
step 1450: loss = 1.5003, val_loss = 1.6914
step 1460: loss = 1.5004, val_loss = 1.6929
step 1470: loss = 1.4921, val_loss = 1.6903
step 1480: loss = 1.4919, val_loss = 1.6844
step 1490: loss = 1.4859, val_loss = 1.6854
step 1500: loss = 1.4842, val_loss = 1.6841
step 1510: loss = 1.4808, val_loss = 1.6751
step 1520: loss = 1.4832, val_loss = 1.6802
step 1530: loss = 1.4768, val_loss = 1.6712
step 1540: loss = 1.4748, val_loss = 1.6725
step 1550: loss = 1.4750, val_loss = 1.6676
step 1560: loss = 1.4708, val_loss = 1.6711
step 1570: loss = 1.4692, val_loss = 1.6634
step 1580: loss = 1.4661, val_loss = 1.6616
step 1590: loss = 1.4655, val_loss = 1.6589
step 1600: loss = 1.4656, val_loss = 1.6576
step 1610: loss = 1.4595, val_loss = 1.6572
step 1620: loss = 1.4598, val_loss = 1.6620
step 1630: loss = 1.4506, val_loss = 1.6553
step 1640: loss = 1.4502, val_loss = 1.6623
step 1650: loss = 1.4500, val_loss = 1.6531
step 1660: loss = 1.4542, val_loss = 1.6625
step 1670: loss = 1.4430, val_loss = 1.6503
step 1680: loss = 1.4437, val_loss = 1.6509
step 1690: loss = 1.4438, val_loss = 1.6523
step 1700: loss = 1.4400, val_loss = 1.6463
step 1710: loss = 1.4360, val_loss = 1.6494
step 1720: loss = 1.4404, val_loss = 1.6483
step 1730: loss = 1.4358, val_loss = 1.6400
step 1740: loss = 1.4295, val_loss = 1.6382
step 1750: loss = 1.4323, val_loss = 1.6381
step 1760: loss = 1.4288, val_loss = 1.6370
step 1770: loss = 1.4255, val_loss = 1.6368
step 1780: loss = 1.4284, val_loss = 1.6322
step 1790: loss = 1.4221, val_loss = 1.6318
step 1800: loss = 1.4247, val_loss = 1.6282
step 1810: loss = 1.4200, val_loss = 1.6320
step 1820: loss = 1.4185, val_loss = 1.6264
step 1830: loss = 1.4155, val_loss = 1.6238
step 1840: loss = 1.4122, val_loss = 1.6239
step 1850: loss = 1.4110, val_loss = 1.6267
step 1860: loss = 1.4099, val_loss = 1.6189
step 1870: loss = 1.4108, val_loss = 1.6268
step 1880: loss = 1.4080, val_loss = 1.6228
step 1890: loss = 1.4000, val_loss = 1.6204
step 1900: loss = 1.4001, val_loss = 1.6176
step 1910: loss = 1.3966, val_loss = 1.6125
step 1920: loss = 1.3937, val_loss = 1.6145
step 1930: loss = 1.3938, val_loss = 1.6136
step 1940: loss = 1.3957, val_loss = 1.6118
step 1950: loss = 1.3936, val_loss = 1.6149
step 1960: loss = 1.3931, val_loss = 1.6158
step 1970: loss = 1.3896, val_loss = 1.6184
step 1980: loss = 1.3876, val_loss = 1.6127
step 1990: loss = 1.3843, val_loss = 1.6078
step 2000: loss = 1.3862, val_loss = 1.6058
step 2010: loss = 1.3827, val_loss = 1.6030
step 2020: loss = 1.3822, val_loss = 1.6027
step 2030: loss = 1.3783, val_loss = 1.5979
step 2040: loss = 1.3801, val_loss = 1.5982
step 2050: loss = 1.3790, val_loss = 1.5983
step 2060: loss = 1.3751, val_loss = 1.5927
step 2070: loss = 1.3734, val_loss = 1.6014
step 2080: loss = 1.3706, val_loss = 1.6008
step 2090: loss = 1.3704, val_loss = 1.5873
step 2100: loss = 1.3675, val_loss = 1.5925
step 2110: loss = 1.3660, val_loss = 1.5815
step 2120: loss = 1.3630, val_loss = 1.5878
step 2130: loss = 1.3688, val_loss = 1.5907
step 2140: loss = 1.3641, val_loss = 1.5918
step 2150: loss = 1.3631, val_loss = 1.5943
step 2160: loss = 1.3577, val_loss = 1.5873
step 2170: loss = 1.3541, val_loss = 1.5819
step 2180: loss = 1.3534, val_loss = 1.5789
step 2190: loss = 1.3525, val_loss = 1.5814
step 2200: loss = 1.3519, val_loss = 1.5791
step 2210: loss = 1.3532, val_loss = 1.5794
step 2220: loss = 1.3529, val_loss = 1.5792
step 2230: loss = 1.3467, val_loss = 1.5740
step 2240: loss = 1.3498, val_loss = 1.5839
step 2250: loss = 1.3472, val_loss = 1.5723
step 2260: loss = 1.3445, val_loss = 1.5687
step 2270: loss = 1.3449, val_loss = 1.5621
step 2280: loss = 1.3433, val_loss = 1.5722
step 2290: loss = 1.3390, val_loss = 1.5638
step 2300: loss = 1.3457, val_loss = 1.5726
step 2310: loss = 1.3385, val_loss = 1.5684
step 2320: loss = 1.3326, val_loss = 1.5668
step 2330: loss = 1.3365, val_loss = 1.5713
step 2340: loss = 1.3322, val_loss = 1.5672
step 2350: loss = 1.3359, val_loss = 1.5703
step 2360: loss = 1.3316, val_loss = 1.5648
step 2370: loss = 1.3297, val_loss = 1.5670
step 2380: loss = 1.3316, val_loss = 1.5701
step 2390: loss = 1.3277, val_loss = 1.5625
step 2400: loss = 1.3251, val_loss = 1.5613
step 2410: loss = 1.3221, val_loss = 1.5576
step 2420: loss = 1.3245, val_loss = 1.5584
step 2430: loss = 1.3220, val_loss = 1.5566
step 2440: loss = 1.3210, val_loss = 1.5564
step 2450: loss = 1.3203, val_loss = 1.5584
step 2460: loss = 1.3188, val_loss = 1.5604
step 2470: loss = 1.3141, val_loss = 1.5526
step 2480: loss = 1.3137, val_loss = 1.5446
step 2490: loss = 1.3116, val_loss = 1.5497
step 2500: loss = 1.3119, val_loss = 1.5515
step 2510: loss = 1.3092, val_loss = 1.5570
step 2520: loss = 1.3128, val_loss = 1.5566
step 2530: loss = 1.3133, val_loss = 1.5553
step 2540: loss = 1.3079, val_loss = 1.5548
step 2550: loss = 1.3089, val_loss = 1.5520
step 2560: loss = 1.3068, val_loss = 1.5463
step 2570: loss = 1.3065, val_loss = 1.5511
step 2580: loss = 1.3101, val_loss = 1.5565
step 2590: loss = 1.3021, val_loss = 1.5459
step 2600: loss = 1.3030, val_loss = 1.5505
step 2610: loss = 1.3030, val_loss = 1.5508
step 2620: loss = 1.2994, val_loss = 1.5414
step 2630: loss = 1.3015, val_loss = 1.5480
step 2640: loss = 1.2985, val_loss = 1.5444
step 2650: loss = 1.2963, val_loss = 1.5467
step 2660: loss = 1.2954, val_loss = 1.5465
step 2670: loss = 1.2985, val_loss = 1.5388
step 2680: loss = 1.2901, val_loss = 1.5343
step 2690: loss = 1.2927, val_loss = 1.5422
step 2700: loss = 1.2914, val_loss = 1.5407
step 2710: loss = 1.2875, val_loss = 1.5343
step 2720: loss = 1.2844, val_loss = 1.5350
step 2730: loss = 1.2879, val_loss = 1.5326
step 2740: loss = 1.2887, val_loss = 1.5365
step 2750: loss = 1.2851, val_loss = 1.5322
step 2760: loss = 1.2848, val_loss = 1.5352
step 2770: loss = 1.2828, val_loss = 1.5320
step 2780: loss = 1.2839, val_loss = 1.5359
step 2790: loss = 1.2831, val_loss = 1.5298
step 2800: loss = 1.2792, val_loss = 1.5317
step 2810: loss = 1.2801, val_loss = 1.5316
step 2820: loss = 1.2779, val_loss = 1.5263
step 2830: loss = 1.2761, val_loss = 1.5302
^[[>0;276;0c^[]11;rgb:ffff/ffff/ffff^[\^[[>0;276;0c^[]11;rgb:ffff/ffff/ffff^[\step 2840: loss = 1.2816, val_loss = 1.5340
step 2850: loss = 1.2784, val_loss = 1.5396
step 2860: loss = 1.2745, val_loss = 1.5321
step 2870: loss = 1.2714, val_loss = 1.5236
step 2880: loss = 1.2722, val_loss = 1.5301
step 2890: loss = 1.2709, val_loss = 1.5253
step 2900: loss = 1.2711, val_loss = 1.5319
step 2910: loss = 1.2684, val_loss = 1.5270
step 2920: loss = 1.2674, val_loss = 1.5235
step 2930: loss = 1.2671, val_loss = 1.5262
step 2940: loss = 1.2629, val_loss = 1.5238
step 2950: loss = 1.2664, val_loss = 1.5266
step 2960: loss = 1.2626, val_loss = 1.5283
step 2970: loss = 1.2602, val_loss = 1.5246
step 2980: loss = 1.2601, val_loss = 1.5239
step 2990: loss = 1.2620, val_loss = 1.5219
step 3000: loss = 1.2558, val_loss = 1.5176
step 3010: loss = 1.2588, val_loss = 1.5203
step 3020: loss = 1.2570, val_loss = 1.5199
step 3030: loss = 1.2615, val_loss = 1.5248
step 3040: loss = 1.2585, val_loss = 1.5203
step 3050: loss = 1.2552, val_loss = 1.5188
step 3060: loss = 1.2562, val_loss = 1.5157
step 3070: loss = 1.2531, val_loss = 1.5192
step 3080: loss = 1.2496, val_loss = 1.5181
step 3090: loss = 1.2492, val_loss = 1.5159
step 3100: loss = 1.2469, val_loss = 1.5114
step 3110: loss = 1.2493, val_loss = 1.5150
step 3120: loss = 1.2507, val_loss = 1.5113
step 3130: loss = 1.2477, val_loss = 1.5133
step 3140: loss = 1.2464, val_loss = 1.5150
step 3150: loss = 1.2492, val_loss = 1.5178
step 3160: loss = 1.2457, val_loss = 1.5133
step 3170: loss = 1.2459, val_loss = 1.5149
step 3180: loss = 1.2452, val_loss = 1.5055
step 3190: loss = 1.2449, val_loss = 1.5019
step 3200: loss = 1.2392, val_loss = 1.5109
step 3210: loss = 1.2390, val_loss = 1.5118
step 3220: loss = 1.2405, val_loss = 1.5101
step 3230: loss = 1.2407, val_loss = 1.5117
step 3240: loss = 1.2381, val_loss = 1.5092
step 3250: loss = 1.2354, val_loss = 1.5079
step 3260: loss = 1.2363, val_loss = 1.5106
step 3270: loss = 1.2338, val_loss = 1.5056
step 3280: loss = 1.2338, val_loss = 1.5100
step 3290: loss = 1.2320, val_loss = 1.5065
step 3300: loss = 1.2326, val_loss = 1.5119
step 3310: loss = 1.2312, val_loss = 1.5083
step 3320: loss = 1.2317, val_loss = 1.5086
step 3330: loss = 1.2307, val_loss = 1.5086
step 3340: loss = 1.2291, val_loss = 1.5031
step 3350: loss = 1.2307, val_loss = 1.5104
step 3360: loss = 1.2283, val_loss = 1.5126
step 3370: loss = 1.2244, val_loss = 1.5100
step 3380: loss = 1.2244, val_loss = 1.5012
step 3390: loss = 1.2240, val_loss = 1.5035
step 3400: loss = 1.2215, val_loss = 1.5026
step 3410: loss = 1.2252, val_loss = 1.5111
step 3420: loss = 1.2208, val_loss = 1.4966
step 3430: loss = 1.2164, val_loss = 1.4965
step 3440: loss = 1.2226, val_loss = 1.5004
step 3450: loss = 1.2189, val_loss = 1.5081
step 3460: loss = 1.2165, val_loss = 1.4934
step 3470: loss = 1.2138, val_loss = 1.4994
step 3480: loss = 1.2173, val_loss = 1.5106
step 3490: loss = 1.2182, val_loss = 1.5094
step 3500: loss = 1.2170, val_loss = 1.5046
step 3510: loss = 1.2161, val_loss = 1.5048
step 3520: loss = 1.2127, val_loss = 1.5080
step 3530: loss = 1.2108, val_loss = 1.4990
step 3540: loss = 1.2132, val_loss = 1.5073
step 3550: loss = 1.2119, val_loss = 1.4951
step 3560: loss = 1.2080, val_loss = 1.5020
step 3570: loss = 1.2085, val_loss = 1.4987
step 3580: loss = 1.2081, val_loss = 1.4984
step 3590: loss = 1.2066, val_loss = 1.4915
step 3600: loss = 1.2051, val_loss = 1.4997
step 3610: loss = 1.2063, val_loss = 1.5041
step 3620: loss = 1.2047, val_loss = 1.5019
step 3630: loss = 1.2075, val_loss = 1.4963
step 3640: loss = 1.2067, val_loss = 1.4977
step 3650: loss = 1.2033, val_loss = 1.4947
step 3660: loss = 1.2014, val_loss = 1.4973
step 3670: loss = 1.2019, val_loss = 1.4918
step 3680: loss = 1.2008, val_loss = 1.4969
step 3690: loss = 1.2000, val_loss = 1.4925
step 3700: loss = 1.2011, val_loss = 1.5009
step 3710: loss = 1.1949, val_loss = 1.4917
step 3720: loss = 1.1964, val_loss = 1.4890
step 3730: loss = 1.1970, val_loss = 1.4933
step 3740: loss = 1.1939, val_loss = 1.4893
step 3750: loss = 1.1942, val_loss = 1.4899
step 3760: loss = 1.1927, val_loss = 1.4998
step 3770: loss = 1.1931, val_loss = 1.4950
step 3780: loss = 1.1923, val_loss = 1.4917
step 3790: loss = 1.1926, val_loss = 1.5029
step 3800: loss = 1.1912, val_loss = 1.4931
step 3810: loss = 1.1901, val_loss = 1.4979
step 3820: loss = 1.1895, val_loss = 1.4941
step 3830: loss = 1.1861, val_loss = 1.4905
step 3840: loss = 1.1874, val_loss = 1.4914
step 3850: loss = 1.1907, val_loss = 1.4964
step 3860: loss = 1.1868, val_loss = 1.4965
step 3870: loss = 1.1835, val_loss = 1.4947
step 3880: loss = 1.1867, val_loss = 1.4946
step 3890: loss = 1.1825, val_loss = 1.4949
step 3900: loss = 1.1835, val_loss = 1.5008
step 3910: loss = 1.1824, val_loss = 1.4854
step 3920: loss = 1.1807, val_loss = 1.4883
step 3930: loss = 1.1789, val_loss = 1.4952
step 3940: loss = 1.1786, val_loss = 1.4885
step 3950: loss = 1.1769, val_loss = 1.4870
step 3960: loss = 1.1749, val_loss = 1.4917
step 3970: loss = 1.1757, val_loss = 1.4868
step 3980: loss = 1.1775, val_loss = 1.4828
step 3990: loss = 1.1762, val_loss = 1.4950
step 4000: loss = 1.1770, val_loss = 1.4899
step 4010: loss = 1.1734, val_loss = 1.4892
step 4020: loss = 1.1725, val_loss = 1.4864
step 4030: loss = 1.1744, val_loss = 1.4916
step 4040: loss = 1.1720, val_loss = 1.4920
step 4050: loss = 1.1711, val_loss = 1.4838
step 4060: loss = 1.1730, val_loss = 1.4972
step 4070: loss = 1.1687, val_loss = 1.4911
step 4080: loss = 1.1674, val_loss = 1.4934
step 4090: loss = 1.1680, val_loss = 1.4873
step 4100: loss = 1.1648, val_loss = 1.4816
step 4110: loss = 1.1651, val_loss = 1.4892
step 4120: loss = 1.1660, val_loss = 1.4813
step 4130: loss = 1.1639, val_loss = 1.4831
step 4140: loss = 1.1631, val_loss = 1.4894
step 4150: loss = 1.1605, val_loss = 1.4892
step 4160: loss = 1.1629, val_loss = 1.4935
step 4170: loss = 1.1612, val_loss = 1.4870
step 4180: loss = 1.1568, val_loss = 1.4893
step 4190: loss = 1.1579, val_loss = 1.4853
step 4200: loss = 1.1599, val_loss = 1.4933
step 4210: loss = 1.1573, val_loss = 1.4888
step 4220: loss = 1.1588, val_loss = 1.4838
step 4230: loss = 1.1560, val_loss = 1.4851
step 4240: loss = 1.1546, val_loss = 1.4786
step 4250: loss = 1.1545, val_loss = 1.4799
step 4260: loss = 1.1546, val_loss = 1.4851
step 4270: loss = 1.1541, val_loss = 1.4813
step 4280: loss = 1.1507, val_loss = 1.4853
step 4290: loss = 1.1523, val_loss = 1.4862
step 4300: loss = 1.1497, val_loss = 1.4826
step 4310: loss = 1.1465, val_loss = 1.4792
step 4320: loss = 1.1503, val_loss = 1.4867
step 4330: loss = 1.1482, val_loss = 1.4950
step 4340: loss = 1.1504, val_loss = 1.4874
step 4350: loss = 1.1468, val_loss = 1.4904
step 4360: loss = 1.1456, val_loss = 1.4879
step 4370: loss = 1.1468, val_loss = 1.4903
step 4380: loss = 1.1464, val_loss = 1.4924
step 4390: loss = 1.1452, val_loss = 1.4953
step 4400: loss = 1.1452, val_loss = 1.4933
step 4410: loss = 1.1419, val_loss = 1.4842
step 4420: loss = 1.1403, val_loss = 1.4864
step 4430: loss = 1.1411, val_loss = 1.4846
step 4440: loss = 1.1424, val_loss = 1.4892
step 4450: loss = 1.1411, val_loss = 1.4841
step 4460: loss = 1.1372, val_loss = 1.4799
step 4470: loss = 1.1365, val_loss = 1.4806
step 4480: loss = 1.1368, val_loss = 1.4806
step 4490: loss = 1.1394, val_loss = 1.4819
step 4500: loss = 1.1367, val_loss = 1.4785
step 4510: loss = 1.1367, val_loss = 1.4895
step 4520: loss = 1.1358, val_loss = 1.4841
step 4530: loss = 1.1349, val_loss = 1.4871
step 4540: loss = 1.1341, val_loss = 1.4824
step 4550: loss = 1.1312, val_loss = 1.4875
step 4560: loss = 1.1322, val_loss = 1.4902
step 4570: loss = 1.1311, val_loss = 1.4727
step 4580: loss = 1.1318, val_loss = 1.4887
step 4590: loss = 1.1300, val_loss = 1.4785
step 4600: loss = 1.1274, val_loss = 1.4804
step 4610: loss = 1.1314, val_loss = 1.4849
step 4620: loss = 1.1290, val_loss = 1.4827
step 4630: loss = 1.1295, val_loss = 1.4835
step 4640: loss = 1.1268, val_loss = 1.4852
step 4650: loss = 1.1267, val_loss = 1.4823
step 4660: loss = 1.1266, val_loss = 1.4823
step 4670: loss = 1.1253, val_loss = 1.4863
step 4680: loss = 1.1240, val_loss = 1.4842
step 4690: loss = 1.1245, val_loss = 1.4813
step 4700: loss = 1.1228, val_loss = 1.4816
step 4710: loss = 1.1229, val_loss = 1.4803
step 4720: loss = 1.1220, val_loss = 1.4801
step 4730: loss = 1.1199, val_loss = 1.4768
step 4740: loss = 1.1194, val_loss = 1.4797
step 4750: loss = 1.1206, val_loss = 1.4794
step 4760: loss = 1.1213, val_loss = 1.4772
step 4770: loss = 1.1214, val_loss = 1.4863
^[[>0;276;0c^[]11;rgb:ffff/ffff/ffff^[\^[[>0;276;0c^[]11;rgb:ffff/ffff/ffff^[\step 4780: loss = 1.1184, val_loss = 1.4799
step 4790: loss = 1.1203, val_loss = 1.4863
step 4800: loss = 1.1148, val_loss = 1.4833
step 4810: loss = 1.1134, val_loss = 1.4872
step 4820: loss = 1.1168, val_loss = 1.4882
step 4830: loss = 1.1153, val_loss = 1.4891
step 4840: loss = 1.1143, val_loss = 1.4782
step 4850: loss = 1.1130, val_loss = 1.4854
step 4860: loss = 1.1094, val_loss = 1.4813
step 4870: loss = 1.1125, val_loss = 1.4884
step 4880: loss = 1.1131, val_loss = 1.4832
step 4890: loss = 1.1100, val_loss = 1.4875
step 4900: loss = 1.1063, val_loss = 1.4883
step 4910: loss = 1.1086, val_loss = 1.4745
step 4920: loss = 1.1044, val_loss = 1.4775
step 4930: loss = 1.1105, val_loss = 1.4840
step 4940: loss = 1.1054, val_loss = 1.4789
step 4950: loss = 1.1062, val_loss = 1.4865
step 4960: loss = 1.1033, val_loss = 1.4782
step 4970: loss = 1.1007, val_loss = 1.4774
step 4980: loss = 1.1028, val_loss = 1.4891
step 4990: loss = 1.1021, val_loss = 1.4804

BUCKINGHAM:
Good and goose ears; which, merry; I must it
seeing for and faith a lamb gentle vow
Page our matress is, no more: pardon me,
but that upon your sigh war usurpy
of himself loss as we are orical: tell the house:
Hoach works you to vengeful of you,
Doesperous before duty, till her, cousin,
their bear of the duke, his is law,
Torried enjoy or them sound, not meet him leanners
'Re-till. Tell her dispeserved thee at thy wiserpring in
That, I had certainly yed!

ROMEO:
I thank you stay fun 
